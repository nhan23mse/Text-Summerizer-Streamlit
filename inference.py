import asyncio
import streamlit as st 
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Fix lá»—i asyncio (RuntimeError: no running event loop)
try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# Hiá»ƒn thá»‹ tiÃªu Ä‘á» trang
st.title("ğŸ“œ AI Text Summarizer")

# Load mÃ´ hÃ¬nh vÃ  tokenizer (dÃ¹ng 't5-small')
@st.cache_resource()
def load_model():
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model = T5ForConditionalGeneration.from_pretrained("t5-small", return_dict=True)
    return tokenizer, model

tokenizer, model = load_model()

# Ã” nháº­p vÄƒn báº£n
text_input = st.text_area("ğŸ“ Nháº­p vÄƒn báº£n cáº§n tÃ³m táº¯t:", height=200)

if st.button("âœ¨ TÃ³m táº¯t ngay"):
    if text_input.strip():
        with st.spinner("ğŸ”„ Äang xá»­ lÃ½..."):
            # MÃ£ hÃ³a vÄƒn báº£n
            tokenized_text = tokenizer.encode_plus(
                text_input, return_attention_mask=True, return_tensors="pt"
            )

            # Sinh vÄƒn báº£n tÃ³m táº¯t
            summary_ids = model.generate(
                input_ids=tokenized_text["input_ids"],
                attention_mask=tokenized_text["attention_mask"],
                max_length=150,
                num_beams=4,
                length_penalty=2.0,
                early_stopping=True
            )

            # Giáº£i mÃ£ káº¿t quáº£
            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        # Hiá»ƒn thá»‹ káº¿t quáº£
        st.subheader("ğŸ“Œ VÄƒn báº£n Ä‘Ã£ tÃ³m táº¯t:")
        st.success(summary)
    else:
        st.warning("âš ï¸ Vui lÃ²ng nháº­p vÄƒn báº£n trÆ°á»›c khi tÃ³m táº¯t!")

# ThÃªm chÃº thÃ­ch
st.markdown("---")
st.caption("ğŸš€ ÄÆ°á»£c há»— trá»£ bá»Ÿi [Hugging Face Transformers](https://huggingface.co/).")
